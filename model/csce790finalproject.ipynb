{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":70082,"databundleVersionId":7671841,"sourceType":"competition"},{"sourceId":8094567,"sourceType":"datasetVersion","datasetId":4779187},{"sourceId":8120654,"sourceType":"datasetVersion","datasetId":4798317},{"sourceId":8149694,"sourceType":"datasetVersion","datasetId":4819801},{"sourceId":8150344,"sourceType":"datasetVersion","datasetId":4820264}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n#from timm.models.efficientnet import efficientnetv2_m\n#import tensorflow.keras.applications.efficientnet as efn\n!pip install timm\nimport timm\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tensorflow as tf\nimport time\n\n\nfrom pathlib import Path\nfrom sklearn.model_selection import KFold, train_test_split\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import to_categorical\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import MeanSquaredError\n \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/vexcompetitionexpanded'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"7iRw7z_IPgoW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"68e9a2eb-7789-44ca-aad4-db0315f109e9","execution":{"iopub.status.busy":"2024-04-21T21:58:26.626054Z","iopub.execute_input":"2024-04-21T21:58:26.626453Z","iopub.status.idle":"2024-04-21T21:59:14.983622Z","shell.execute_reply.started":"2024-04-21T21:58:26.626404Z","shell.execute_reply":"2024-04-21T21:59:14.982478Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (0.9.16)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from timm) (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm) (0.16.2)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm) (6.0.1)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from timm) (0.22.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.4.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (4.66.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->timm) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub->timm) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\n","output_type":"stream"},{"name":"stderr","text":"2024-04-21 21:59:00.199460: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-21 21:59:00.199588: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-21 21:59:00.489452: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/vexcompetitionexpanded/notes.json\n/kaggle/input/vexcompetitionexpanded/classes.txt\n/kaggle/input/vexcompetitionexpanded/labels/f45f738c-data1.txt\n/kaggle/input/vexcompetitionexpanded/labels/98c6b011-data109.txt\n/kaggle/input/vexcompetitionexpanded/labels/b7daf22a-data118.txt\n/kaggle/input/vexcompetitionexpanded/labels/f0e10d90-data20.txt\n/kaggle/input/vexcompetitionexpanded/labels/210c603a-data138.txt\n/kaggle/input/vexcompetitionexpanded/labels/f19d8fd0-data51.txt\n/kaggle/input/vexcompetitionexpanded/labels/7278e187-data111.txt\n/kaggle/input/vexcompetitionexpanded/labels/c6a53bb1-data105.txt\n/kaggle/input/vexcompetitionexpanded/labels/b7348070-data142.txt\n/kaggle/input/vexcompetitionexpanded/labels/37575e82-data32.txt\n/kaggle/input/vexcompetitionexpanded/labels/00ae3865-data133.txt\n/kaggle/input/vexcompetitionexpanded/labels/6bf453c8-data4.txt\n/kaggle/input/vexcompetitionexpanded/labels/5289065a-data147.txt\n/kaggle/input/vexcompetitionexpanded/labels/1524c45b-data14.txt\n/kaggle/input/vexcompetitionexpanded/labels/afde19e1-data41.txt\n/kaggle/input/vexcompetitionexpanded/labels/3983c195-data18.txt\n/kaggle/input/vexcompetitionexpanded/labels/328b2f22-data123.txt\n/kaggle/input/vexcompetitionexpanded/labels/880efabb-data46.txt\n/kaggle/input/vexcompetitionexpanded/labels/20a28b22-data128.txt\n/kaggle/input/vexcompetitionexpanded/labels/7c9d39d1-data12.txt\n/kaggle/input/vexcompetitionexpanded/labels/243d8a83-data144.txt\n/kaggle/input/vexcompetitionexpanded/labels/479d8d36-data135.txt\n/kaggle/input/vexcompetitionexpanded/labels/145ea51b-data119.txt\n/kaggle/input/vexcompetitionexpanded/labels/df737d2e-data103.txt\n/kaggle/input/vexcompetitionexpanded/labels/b2204f77-data116.txt\n/kaggle/input/vexcompetitionexpanded/labels/0707d599-data107.txt\n/kaggle/input/vexcompetitionexpanded/labels/467e1551-data125.txt\n/kaggle/input/vexcompetitionexpanded/labels/b6e975c6-data104.txt\n/kaggle/input/vexcompetitionexpanded/labels/64dca41f-data49.txt\n/kaggle/input/vexcompetitionexpanded/labels/45de3936-data45.txt\n/kaggle/input/vexcompetitionexpanded/labels/f75582ad-data25.txt\n/kaggle/input/vexcompetitionexpanded/labels/9af5dfd8-data24.txt\n/kaggle/input/vexcompetitionexpanded/labels/bb83ebeb-data29.txt\n/kaggle/input/vexcompetitionexpanded/labels/2faad2ae-data37.txt\n/kaggle/input/vexcompetitionexpanded/labels/40237789-data52.txt\n/kaggle/input/vexcompetitionexpanded/labels/fe4e864a-data130.txt\n/kaggle/input/vexcompetitionexpanded/labels/7992e406-data26.txt\n/kaggle/input/vexcompetitionexpanded/labels/3da2da0e-data148.txt\n/kaggle/input/vexcompetitionexpanded/labels/c7b85582-data30.txt\n/kaggle/input/vexcompetitionexpanded/labels/ab9b9b1b-data122.txt\n/kaggle/input/vexcompetitionexpanded/labels/ad6c2e92-data124.txt\n/kaggle/input/vexcompetitionexpanded/labels/361265a1-data44.txt\n/kaggle/input/vexcompetitionexpanded/labels/fbb721d8-data21.txt\n/kaggle/input/vexcompetitionexpanded/labels/efb956d5-data146.txt\n/kaggle/input/vexcompetitionexpanded/labels/87ce836e-data48.txt\n/kaggle/input/vexcompetitionexpanded/labels/904daf2d-data150.txt\n/kaggle/input/vexcompetitionexpanded/labels/f9abb60a-data129.txt\n/kaggle/input/vexcompetitionexpanded/labels/3fe9091b-data120.txt\n/kaggle/input/vexcompetitionexpanded/labels/b97963d7-data132.txt\n/kaggle/input/vexcompetitionexpanded/labels/92a2abd9-data102.txt\n/kaggle/input/vexcompetitionexpanded/labels/cd536079-data13.txt\n/kaggle/input/vexcompetitionexpanded/labels/64f42c7e-data31.txt\n/kaggle/input/vexcompetitionexpanded/labels/643b971f-data43.txt\n/kaggle/input/vexcompetitionexpanded/labels/ea121df0-data23.txt\n/kaggle/input/vexcompetitionexpanded/labels/44eec2f9-data27.txt\n/kaggle/input/vexcompetitionexpanded/labels/d85b543c-data127.txt\n/kaggle/input/vexcompetitionexpanded/labels/eda73943-data40.txt\n/kaggle/input/vexcompetitionexpanded/labels/d2a597f0-data113.txt\n/kaggle/input/vexcompetitionexpanded/labels/37bf0aa4-data143.txt\n/kaggle/input/vexcompetitionexpanded/labels/941f749c-data2.txt\n/kaggle/input/vexcompetitionexpanded/labels/ab5422e6-data101.txt\n/kaggle/input/vexcompetitionexpanded/labels/55ba5e17-data17.txt\n/kaggle/input/vexcompetitionexpanded/labels/4117f5db-data106.txt\n/kaggle/input/vexcompetitionexpanded/labels/d0658f5c-data22.txt\n/kaggle/input/vexcompetitionexpanded/labels/87e440e3-data112.txt\n/kaggle/input/vexcompetitionexpanded/labels/229d24e8-data149.txt\n/kaggle/input/vexcompetitionexpanded/labels/53421b42-data35.txt\n/kaggle/input/vexcompetitionexpanded/labels/e4392c7e-data115.txt\n/kaggle/input/vexcompetitionexpanded/labels/e914504d-data145.txt\n/kaggle/input/vexcompetitionexpanded/labels/1e6e129a-data131.txt\n/kaggle/input/vexcompetitionexpanded/labels/6f4940ab-data39.txt\n/kaggle/input/vexcompetitionexpanded/labels/db6b2fef-data33.txt\n/kaggle/input/vexcompetitionexpanded/labels/f5a2b7fe-data136.txt\n/kaggle/input/vexcompetitionexpanded/labels/e126a18c-data28.txt\n/kaggle/input/vexcompetitionexpanded/labels/7b9460a9-data38.txt\n/kaggle/input/vexcompetitionexpanded/labels/49d2b4b5-data50.txt\n/kaggle/input/vexcompetitionexpanded/labels/dc61cf21-data140.txt\n/kaggle/input/vexcompetitionexpanded/labels/3947b4d9-data47.txt\n/kaggle/input/vexcompetitionexpanded/labels/d2e86463-data42.txt\n/kaggle/input/vexcompetitionexpanded/labels/bb699861-data5.txt\n/kaggle/input/vexcompetitionexpanded/labels/af58eb43-data114.txt\n/kaggle/input/vexcompetitionexpanded/labels/21d782fb-data117.txt\n/kaggle/input/vexcompetitionexpanded/labels/f7f17231-data100.txt\n/kaggle/input/vexcompetitionexpanded/labels/cf2ba5df-data126.txt\n/kaggle/input/vexcompetitionexpanded/labels/46666ce7-data121.txt\n/kaggle/input/vexcompetitionexpanded/labels/4da430ac-data19.txt\n/kaggle/input/vexcompetitionexpanded/labels/cdcb8bce-data3.txt\n/kaggle/input/vexcompetitionexpanded/labels/202cdb09-data10.txt\n/kaggle/input/vexcompetitionexpanded/labels/6cacb8c8-data0.txt\n/kaggle/input/vexcompetitionexpanded/labels/5bd00f31-data11.txt\n/kaggle/input/vexcompetitionexpanded/labels/957ee46a-data16.txt\n/kaggle/input/vexcompetitionexpanded/labels/7bd42189-data139.txt\n/kaggle/input/vexcompetitionexpanded/labels/b56d56a2-data108.txt\n/kaggle/input/vexcompetitionexpanded/labels/1d5cd533-data137.txt\n/kaggle/input/vexcompetitionexpanded/labels/684b297b-data141.txt\n/kaggle/input/vexcompetitionexpanded/labels/acbbc3f5-data134.txt\n/kaggle/input/vexcompetitionexpanded/labels/4b105bfb-data15.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/notes.json\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/classes.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/f45f738c-data1.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/98c6b011-data109.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/fe93634d-data11.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/b7daf22a-data118.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/f0e10d90-data20.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/210c603a-data138.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/f19d8fd0-data51.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/7278e187-data111.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/207d9fe1-data21.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/3dc316dc-data10.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/c6a53bb1-data105.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/b7348070-data142.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/37575e82-data32.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/00ae3865-data133.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/6bf453c8-data4.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/5289065a-data147.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/1524c45b-data14.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/afde19e1-data41.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/3983c195-data18.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/328b2f22-data123.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/880efabb-data46.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/20a28b22-data128.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/1fc36643-data4.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/7c9d39d1-data12.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/243d8a83-data144.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/2de035ba-data9.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/f41ffa5b-data5.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/479d8d36-data135.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/53421f01-data26.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/145ea51b-data119.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/df737d2e-data103.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/b2204f77-data116.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/0707d599-data107.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/467e1551-data125.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/b9fd22f7-data15.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/b6e975c6-data104.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/7b838954-data17.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/64dca41f-data49.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/45de3936-data45.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/f75582ad-data25.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/9af5dfd8-data24.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/bb83ebeb-data29.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/af94f6e7-data22.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/2faad2ae-data37.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/40237789-data52.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/fe4e864a-data130.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/7992e406-data26.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/6afee809-data25.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/3da2da0e-data148.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/c7b85582-data30.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/ab9b9b1b-data122.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/ad6c2e92-data124.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/361265a1-data44.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/fbb721d8-data21.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/efb956d5-data146.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/de6ab6e1-data14.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/87ce836e-data48.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/904daf2d-data150.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/f9abb60a-data129.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/3fe9091b-data120.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/b97963d7-data132.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/92a2abd9-data102.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/e28e1e70-data16.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/cd536079-data13.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/64f42c7e-data31.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/643b971f-data43.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/ea121df0-data23.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/44eec2f9-data27.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/d85b543c-data127.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/eda73943-data40.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/d2a597f0-data113.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/37bf0aa4-data143.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/941f749c-data2.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/ab5422e6-data101.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/55ba5e17-data17.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/34e6a65f-data24.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/59cf33c1-data7.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/a65879d7-data8.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/4117f5db-data106.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/d0658f5c-data22.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/24af8c89-data6.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/87e440e3-data112.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/229d24e8-data149.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/53421b42-data35.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/e4392c7e-data115.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/e914504d-data145.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/1e6e129a-data131.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/6f4940ab-data39.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/db6b2fef-data33.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/f5a2b7fe-data136.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/e126a18c-data28.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/400cee38-data12.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/7b9460a9-data38.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/49d2b4b5-data50.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/dc61cf21-data140.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/3947b4d9-data47.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/d2e86463-data42.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/bb699861-data5.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/af58eb43-data114.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/21d782fb-data117.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/f7f17231-data100.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/cf2ba5df-data126.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/46666ce7-data121.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/4da430ac-data19.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/cdcb8bce-data3.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/202cdb09-data10.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/6cacb8c8-data0.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/5bd00f31-data11.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/06db08da-data13.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/957ee46a-data16.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/7bd42189-data139.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/b56d56a2-data108.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/1d5cd533-data137.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/684b297b-data141.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/acbbc3f5-data134.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels/4b105bfb-data15.txt\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/ad6c2e92-data124.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/a65879d7-data8.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/d0658f5c-data22.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/b7daf22a-data118.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/b97963d7-data132.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/f5a2b7fe-data136.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/37575e82-data32.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/f19d8fd0-data51.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/f75582ad-data25.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/46666ce7-data121.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/b7348070-data142.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/0707d599-data107.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/1524c45b-data14.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/3dc316dc-data10.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/1e6e129a-data131.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/6cacb8c8-data0.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/6bf453c8-data4.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/e126a18c-data28.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/b2204f77-data116.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/44eec2f9-data27.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/ea121df0-data23.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/53421b42-data35.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/f0e10d90-data20.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/db6b2fef-data33.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/efb956d5-data146.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/34e6a65f-data24.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/21d782fb-data117.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/87e440e3-data112.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/4da430ac-data19.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/24af8c89-data6.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/55ba5e17-data17.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/e914504d-data145.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/b6e975c6-data104.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/d85b543c-data127.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/3983c195-data18.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/dc61cf21-data140.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/f45f738c-data1.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/37bf0aa4-data143.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/ab9b9b1b-data122.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/361265a1-data44.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/684b297b-data141.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/3da2da0e-data148.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/210c603a-data138.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/3fe9091b-data120.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/fe4e864a-data130.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/479d8d36-data135.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/af94f6e7-data22.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/2de035ba-data9.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/40237789-data52.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/4117f5db-data106.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/7c9d39d1-data12.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/467e1551-data125.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/cdcb8bce-data3.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/400cee38-data12.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/92a2abd9-data102.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/145ea51b-data119.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/c7b85582-data30.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/cd536079-data13.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/e4392c7e-data115.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/cf2ba5df-data126.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/202cdb09-data10.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/f9abb60a-data129.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/2faad2ae-data37.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/6f4940ab-data39.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/5bd00f31-data11.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/d2e86463-data42.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/e28e1e70-data16.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/00ae3865-data133.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/df737d2e-data103.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/d2a597f0-data113.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/de6ab6e1-data14.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/53421f01-data26.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/45de3936-data45.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/64dca41f-data49.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/1fc36643-data4.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/87ce836e-data48.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/49d2b4b5-data50.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/98c6b011-data109.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/1d5cd533-data137.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/c6a53bb1-data105.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/afde19e1-data41.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/229d24e8-data149.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/b56d56a2-data108.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/bb83ebeb-data29.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/643b971f-data43.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/4b105bfb-data15.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/af58eb43-data114.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/9af5dfd8-data24.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/b9fd22f7-data15.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/243d8a83-data144.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/941f749c-data2.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/f41ffa5b-data5.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/acbbc3f5-data134.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/3947b4d9-data47.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/7bd42189-data139.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/f7f17231-data100.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/328b2f22-data123.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/904daf2d-data150.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/64f42c7e-data31.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/7b838954-data17.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/06db08da-data13.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/6afee809-data25.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/7278e187-data111.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/7992e406-data26.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/7b9460a9-data38.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/fbb721d8-data21.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/20a28b22-data128.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/957ee46a-data16.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/fe93634d-data11.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/207d9fe1-data21.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/880efabb-data46.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/5289065a-data147.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/ab5422e6-data101.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/59cf33c1-data7.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/bb699861-data5.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/eda73943-data40.jpg\n/kaggle/input/vexcompetitionexpanded/images/ad6c2e92-data124.jpg\n/kaggle/input/vexcompetitionexpanded/images/d0658f5c-data22.jpg\n/kaggle/input/vexcompetitionexpanded/images/b7daf22a-data118.jpg\n/kaggle/input/vexcompetitionexpanded/images/b97963d7-data132.jpg\n/kaggle/input/vexcompetitionexpanded/images/f5a2b7fe-data136.jpg\n/kaggle/input/vexcompetitionexpanded/images/37575e82-data32.jpg\n/kaggle/input/vexcompetitionexpanded/images/f19d8fd0-data51.jpg\n/kaggle/input/vexcompetitionexpanded/images/f75582ad-data25.jpg\n/kaggle/input/vexcompetitionexpanded/images/46666ce7-data121.jpg\n/kaggle/input/vexcompetitionexpanded/images/b7348070-data142.jpg\n/kaggle/input/vexcompetitionexpanded/images/0707d599-data107.jpg\n/kaggle/input/vexcompetitionexpanded/images/1524c45b-data14.jpg\n/kaggle/input/vexcompetitionexpanded/images/1e6e129a-data131.jpg\n/kaggle/input/vexcompetitionexpanded/images/6cacb8c8-data0.jpg\n/kaggle/input/vexcompetitionexpanded/images/6bf453c8-data4.jpg\n/kaggle/input/vexcompetitionexpanded/images/e126a18c-data28.jpg\n/kaggle/input/vexcompetitionexpanded/images/b2204f77-data116.jpg\n/kaggle/input/vexcompetitionexpanded/images/44eec2f9-data27.jpg\n/kaggle/input/vexcompetitionexpanded/images/ea121df0-data23.jpg\n/kaggle/input/vexcompetitionexpanded/images/53421b42-data35.jpg\n/kaggle/input/vexcompetitionexpanded/images/f0e10d90-data20.jpg\n/kaggle/input/vexcompetitionexpanded/images/db6b2fef-data33.jpg\n/kaggle/input/vexcompetitionexpanded/images/efb956d5-data146.jpg\n/kaggle/input/vexcompetitionexpanded/images/21d782fb-data117.jpg\n/kaggle/input/vexcompetitionexpanded/images/87e440e3-data112.jpg\n/kaggle/input/vexcompetitionexpanded/images/4da430ac-data19.jpg\n/kaggle/input/vexcompetitionexpanded/images/55ba5e17-data17.jpg\n/kaggle/input/vexcompetitionexpanded/images/e914504d-data145.jpg\n/kaggle/input/vexcompetitionexpanded/images/b6e975c6-data104.jpg\n/kaggle/input/vexcompetitionexpanded/images/d85b543c-data127.jpg\n/kaggle/input/vexcompetitionexpanded/images/3983c195-data18.jpg\n/kaggle/input/vexcompetitionexpanded/images/dc61cf21-data140.jpg\n/kaggle/input/vexcompetitionexpanded/images/f45f738c-data1.jpg\n/kaggle/input/vexcompetitionexpanded/images/37bf0aa4-data143.jpg\n/kaggle/input/vexcompetitionexpanded/images/ab9b9b1b-data122.jpg\n/kaggle/input/vexcompetitionexpanded/images/361265a1-data44.jpg\n/kaggle/input/vexcompetitionexpanded/images/684b297b-data141.jpg\n/kaggle/input/vexcompetitionexpanded/images/3da2da0e-data148.jpg\n/kaggle/input/vexcompetitionexpanded/images/210c603a-data138.jpg\n/kaggle/input/vexcompetitionexpanded/images/3fe9091b-data120.jpg\n/kaggle/input/vexcompetitionexpanded/images/fe4e864a-data130.jpg\n/kaggle/input/vexcompetitionexpanded/images/479d8d36-data135.jpg\n/kaggle/input/vexcompetitionexpanded/images/40237789-data52.jpg\n/kaggle/input/vexcompetitionexpanded/images/4117f5db-data106.jpg\n/kaggle/input/vexcompetitionexpanded/images/7c9d39d1-data12.jpg\n/kaggle/input/vexcompetitionexpanded/images/467e1551-data125.jpg\n/kaggle/input/vexcompetitionexpanded/images/cdcb8bce-data3.jpg\n/kaggle/input/vexcompetitionexpanded/images/92a2abd9-data102.jpg\n/kaggle/input/vexcompetitionexpanded/images/145ea51b-data119.jpg\n/kaggle/input/vexcompetitionexpanded/images/c7b85582-data30.jpg\n/kaggle/input/vexcompetitionexpanded/images/cd536079-data13.jpg\n/kaggle/input/vexcompetitionexpanded/images/e4392c7e-data115.jpg\n/kaggle/input/vexcompetitionexpanded/images/cf2ba5df-data126.jpg\n/kaggle/input/vexcompetitionexpanded/images/202cdb09-data10.jpg\n/kaggle/input/vexcompetitionexpanded/images/f9abb60a-data129.jpg\n/kaggle/input/vexcompetitionexpanded/images/2faad2ae-data37.jpg\n/kaggle/input/vexcompetitionexpanded/images/6f4940ab-data39.jpg\n/kaggle/input/vexcompetitionexpanded/images/5bd00f31-data11.jpg\n/kaggle/input/vexcompetitionexpanded/images/d2e86463-data42.jpg\n/kaggle/input/vexcompetitionexpanded/images/00ae3865-data133.jpg\n/kaggle/input/vexcompetitionexpanded/images/df737d2e-data103.jpg\n/kaggle/input/vexcompetitionexpanded/images/d2a597f0-data113.jpg\n/kaggle/input/vexcompetitionexpanded/images/45de3936-data45.jpg\n/kaggle/input/vexcompetitionexpanded/images/64dca41f-data49.jpg\n/kaggle/input/vexcompetitionexpanded/images/87ce836e-data48.jpg\n/kaggle/input/vexcompetitionexpanded/images/49d2b4b5-data50.jpg\n/kaggle/input/vexcompetitionexpanded/images/98c6b011-data109.jpg\n/kaggle/input/vexcompetitionexpanded/images/1d5cd533-data137.jpg\n/kaggle/input/vexcompetitionexpanded/images/c6a53bb1-data105.jpg\n/kaggle/input/vexcompetitionexpanded/images/afde19e1-data41.jpg\n/kaggle/input/vexcompetitionexpanded/images/229d24e8-data149.jpg\n/kaggle/input/vexcompetitionexpanded/images/b56d56a2-data108.jpg\n/kaggle/input/vexcompetitionexpanded/images/bb83ebeb-data29.jpg\n/kaggle/input/vexcompetitionexpanded/images/643b971f-data43.jpg\n/kaggle/input/vexcompetitionexpanded/images/4b105bfb-data15.jpg\n/kaggle/input/vexcompetitionexpanded/images/af58eb43-data114.jpg\n/kaggle/input/vexcompetitionexpanded/images/9af5dfd8-data24.jpg\n/kaggle/input/vexcompetitionexpanded/images/243d8a83-data144.jpg\n/kaggle/input/vexcompetitionexpanded/images/941f749c-data2.jpg\n/kaggle/input/vexcompetitionexpanded/images/acbbc3f5-data134.jpg\n/kaggle/input/vexcompetitionexpanded/images/3947b4d9-data47.jpg\n/kaggle/input/vexcompetitionexpanded/images/7bd42189-data139.jpg\n/kaggle/input/vexcompetitionexpanded/images/f7f17231-data100.jpg\n/kaggle/input/vexcompetitionexpanded/images/328b2f22-data123.jpg\n/kaggle/input/vexcompetitionexpanded/images/904daf2d-data150.jpg\n/kaggle/input/vexcompetitionexpanded/images/64f42c7e-data31.jpg\n/kaggle/input/vexcompetitionexpanded/images/7278e187-data111.jpg\n/kaggle/input/vexcompetitionexpanded/images/7992e406-data26.jpg\n/kaggle/input/vexcompetitionexpanded/images/7b9460a9-data38.jpg\n/kaggle/input/vexcompetitionexpanded/images/fbb721d8-data21.jpg\n/kaggle/input/vexcompetitionexpanded/images/20a28b22-data128.jpg\n/kaggle/input/vexcompetitionexpanded/images/957ee46a-data16.jpg\n/kaggle/input/vexcompetitionexpanded/images/880efabb-data46.jpg\n/kaggle/input/vexcompetitionexpanded/images/5289065a-data147.jpg\n/kaggle/input/vexcompetitionexpanded/images/ab5422e6-data101.jpg\n/kaggle/input/vexcompetitionexpanded/images/bb699861-data5.jpg\n/kaggle/input/vexcompetitionexpanded/images/eda73943-data40.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"all_devices = tf.config.list_physical_devices()\nall_devices","metadata":{"id":"DKYeRuAcPgoY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c1c0f9ab-1bfa-45bf-c6d8-0059862104cb","execution":{"iopub.status.busy":"2024-04-21T21:59:14.985466Z","iopub.execute_input":"2024-04-21T21:59:14.986091Z","iopub.status.idle":"2024-04-21T21:59:15.080615Z","shell.execute_reply.started":"2024-04-21T21:59:14.986060Z","shell.execute_reply":"2024-04-21T21:59:15.079337Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"},"metadata":{}}]},{"cell_type":"code","source":"if 'GPU' in str(all_devices):\n    # Use GPUs for training/inference\n    devices = tf.config.experimental.list_physical_devices('GPU')\n    tf.config.experimental.list_physical_devices()\n    device_names = [d.name.split('e:')[1] for d in devices]\n    # Multiple GPUS\n    #strategy = tf.distribute.MirroredStrategy(devices=device_names[:len(devices)])\n    # One GPU\n    strategy = tf.distribute.OneDeviceStrategy(device_names[0])\n    print(f'Using GPU for training...')\n    device_name_for_file = f'GPU_x{len(device_names)}'\nelif 'TPU' in str(all_devices):\n    # Use TPU for training\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print(f'Using TPUs for training...')\n    device_name_for_file = 'TPU'\nelse:\n    # Run on CPU\n    devices = tf.config.experimental.list_physical_devices('CPU')\n    device_names = [d.name.split('e:')[1] for d in devices]\n    strategy = tf.distribute.OneDeviceStrategy(device_names[0])\n    print(f'Using CPUs for training...')\n    device_name_for_file = 'CPU'\n\n","metadata":{"id":"v_oetFuJPgoZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3b1b8f49-c79f-4c29-be83-1254f02b7202","execution":{"iopub.status.busy":"2024-04-21T21:59:15.087034Z","iopub.execute_input":"2024-04-21T21:59:15.087464Z","iopub.status.idle":"2024-04-21T21:59:15.437892Z","shell.execute_reply.started":"2024-04-21T21:59:15.087416Z","shell.execute_reply":"2024-04-21T21:59:15.436524Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Using GPU for training...\n","output_type":"stream"}]},{"cell_type":"code","source":"# If mnist_train.csv and mnist_test.csv are not /kaggle/input,\n# then specify the correct directory to where they are located\n!ls /kaggle/input/assignment-3-spring2024\ndataset_location = Path('/kaggle/input/Yolo-labels-4_12_24')","metadata":{"id":"Dk8UEL-JPgoa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"92e71e11-0fb1-4dc7-e7c8-44434fdecade","execution":{"iopub.status.busy":"2024-04-21T21:59:15.439269Z","iopub.execute_input":"2024-04-21T21:59:15.440316Z","iopub.status.idle":"2024-04-21T21:59:16.496419Z","shell.execute_reply.started":"2024-04-21T21:59:15.440276Z","shell.execute_reply":"2024-04-21T21:59:16.495269Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"sampleSubmission.csv  test_images.npy  train_images.npy  train_labels.npy\n","output_type":"stream"}]},{"cell_type":"code","source":"train_images_dir = '/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images'\ntrain_labels_dir = '/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/labels'","metadata":{"id":"6q2OPP9yPgog","execution":{"iopub.status.busy":"2024-04-21T21:59:16.498196Z","iopub.execute_input":"2024-04-21T21:59:16.498714Z","iopub.status.idle":"2024-04-21T21:59:16.503880Z","shell.execute_reply.started":"2024-04-21T21:59:16.498675Z","shell.execute_reply":"2024-04-21T21:59:16.502873Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"'''\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # dimesion reduction\n        rotation_range=2,  # randomly rotate images in the range 5 degrees\n        zoom_range = 0.01, # Randomly zoom image 10%\n        width_shift_range=0.01,  # randomly shift images horizontally 10%\n        height_shift_range=0.01,  # randomly shift images vertically 10%\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(train_images_dir)\n'''","metadata":{"id":"NIdC8R5yPgog","colab":{"base_uri":"https://localhost:8080/","height":89},"outputId":"02ae5d3e-82c7-4d8e-da6b-eccb186778d5","execution":{"iopub.status.busy":"2024-04-21T21:59:16.505640Z","iopub.execute_input":"2024-04-21T21:59:16.506274Z","iopub.status.idle":"2024-04-21T21:59:16.550677Z","shell.execute_reply.started":"2024-04-21T21:59:16.505991Z","shell.execute_reply":"2024-04-21T21:59:16.549645Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'\\ndatagen = ImageDataGenerator(\\n        featurewise_center=False,  # set input mean to 0 over the dataset\\n        samplewise_center=False,  # set each sample mean to 0\\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\\n        samplewise_std_normalization=False,  # divide each input by its std\\n        zca_whitening=False,  # dimesion reduction\\n        rotation_range=2,  # randomly rotate images in the range 5 degrees\\n        zoom_range = 0.01, # Randomly zoom image 10%\\n        width_shift_range=0.01,  # randomly shift images horizontally 10%\\n        height_shift_range=0.01,  # randomly shift images vertically 10%\\n        horizontal_flip=False,  # randomly flip images\\n        vertical_flip=False)  # randomly flip images\\n\\ndatagen.fit(train_images_dir)\\n'"},"metadata":{}}]},{"cell_type":"code","source":"import cv2\n#private array of image and annotation paths\nimage_paths = []\nlabel_paths = []\n\n# Iterate over the image directory\nfor img_path in os.listdir(train_images_dir):\n    image_path = os.path.join(train_images_dir, img_path)\n    print(image_path)\n    #form annotation path from annotation directory then the first part of the image name without extension, then add .txt extension\n    annotation_path = os.path.join(train_labels_dir, os.path.splitext(img_path)[0] + '.txt')\n    #print(annotation_path)\n    #if os.path.isfile(image_path) and os.path.isfile(annotation_path):#if the files exist at both places\n    image_paths.append(image_path)\n    label_paths.append(annotation_path)\n        #add them to list\n#index in index array\n\n\nnp.array(image_paths).shape\nnp.array(label_paths).shape","metadata":{"id":"vSc156cGPgoh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0895f7d2-5f98-4922-d308-042afdcf531c","execution":{"iopub.status.busy":"2024-04-21T21:59:16.552091Z","iopub.execute_input":"2024-04-21T21:59:16.552804Z","iopub.status.idle":"2024-04-21T21:59:17.009252Z","shell.execute_reply.started":"2024-04-21T21:59:16.552766Z","shell.execute_reply":"2024-04-21T21:59:17.008327Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/ad6c2e92-data124.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/a65879d7-data8.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/d0658f5c-data22.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/b7daf22a-data118.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/b97963d7-data132.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/f5a2b7fe-data136.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/37575e82-data32.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/f19d8fd0-data51.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/f75582ad-data25.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/46666ce7-data121.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/b7348070-data142.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/0707d599-data107.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/1524c45b-data14.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/3dc316dc-data10.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/1e6e129a-data131.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/6cacb8c8-data0.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/6bf453c8-data4.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/e126a18c-data28.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/b2204f77-data116.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/44eec2f9-data27.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/ea121df0-data23.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/53421b42-data35.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/f0e10d90-data20.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/db6b2fef-data33.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/efb956d5-data146.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/34e6a65f-data24.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/21d782fb-data117.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/87e440e3-data112.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/4da430ac-data19.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/24af8c89-data6.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/55ba5e17-data17.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/e914504d-data145.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/b6e975c6-data104.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/d85b543c-data127.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/3983c195-data18.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/dc61cf21-data140.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/f45f738c-data1.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/37bf0aa4-data143.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/ab9b9b1b-data122.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/361265a1-data44.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/684b297b-data141.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/3da2da0e-data148.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/210c603a-data138.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/3fe9091b-data120.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/fe4e864a-data130.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/479d8d36-data135.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/af94f6e7-data22.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/2de035ba-data9.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/40237789-data52.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/4117f5db-data106.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/7c9d39d1-data12.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/467e1551-data125.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/cdcb8bce-data3.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/400cee38-data12.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/92a2abd9-data102.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/145ea51b-data119.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/c7b85582-data30.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/cd536079-data13.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/e4392c7e-data115.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/cf2ba5df-data126.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/202cdb09-data10.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/f9abb60a-data129.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/2faad2ae-data37.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/6f4940ab-data39.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/5bd00f31-data11.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/d2e86463-data42.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/e28e1e70-data16.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/00ae3865-data133.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/df737d2e-data103.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/d2a597f0-data113.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/de6ab6e1-data14.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/53421f01-data26.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/45de3936-data45.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/64dca41f-data49.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/1fc36643-data4.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/87ce836e-data48.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/49d2b4b5-data50.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/98c6b011-data109.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/1d5cd533-data137.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/c6a53bb1-data105.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/afde19e1-data41.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/229d24e8-data149.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/b56d56a2-data108.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/bb83ebeb-data29.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/643b971f-data43.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/4b105bfb-data15.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/af58eb43-data114.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/9af5dfd8-data24.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/b9fd22f7-data15.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/243d8a83-data144.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/941f749c-data2.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/f41ffa5b-data5.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/acbbc3f5-data134.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/3947b4d9-data47.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/7bd42189-data139.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/f7f17231-data100.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/328b2f22-data123.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/904daf2d-data150.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/64f42c7e-data31.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/7b838954-data17.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/06db08da-data13.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/6afee809-data25.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/7278e187-data111.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/7992e406-data26.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/7b9460a9-data38.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/fbb721d8-data21.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/20a28b22-data128.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/957ee46a-data16.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/fe93634d-data11.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/207d9fe1-data21.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/880efabb-data46.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/5289065a-data147.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/ab5422e6-data101.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/59cf33c1-data7.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/bb699861-data5.jpg\n/kaggle/input/vexcompetitionexpanded/Yolo-labels-4_12_24/images/eda73943-data40.jpg\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(116,)"},"metadata":{}}]},{"cell_type":"code","source":"images = []\n\n\n#for every image in batch get the image\nfor path in image_paths:\n    img = cv2.imread(path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    images.append(img)\n\ntrain_images = np.array(images)\ntrain_images.shape","metadata":{"id":"GNtgVPwOfHSK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0cc2e89c-1163-4005-bc35-fdba3a84c405","execution":{"iopub.status.busy":"2024-04-21T21:59:17.010554Z","iopub.execute_input":"2024-04-21T21:59:17.010856Z","iopub.status.idle":"2024-04-21T21:59:18.934118Z","shell.execute_reply.started":"2024-04-21T21:59:17.010831Z","shell.execute_reply":"2024-04-21T21:59:18.933139Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(116, 480, 640, 3)"},"metadata":{}}]},{"cell_type":"code","source":"bboxes = []\nfor path in label_paths:\n    #print(path)\n    #open annotation path, turn into a bounding box array of floats\n    with open(path, 'r') as f:\n        lines = f.readlines()\n        if(lines == []):\n          #THis is if there is not an object\n          #this solution sucks but stops crashes\n          #ideal is to hhave the bot move straight in this case, or maybe turn\n          #not sure what to put here\n          bbox = [0,0,0,0,0]\n          bboxes.append(bbox)\n        else:\n          bbox = [float(x) for x in lines[0].split(\" \")]\n          bboxes.append(bbox)\n#convert to numpy array and return\ntrain_labels = np.array(bboxes)","metadata":{"id":"8HO3clvWfGDc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c51ced61-1c79-43bf-dbcd-4046ed6ae3b7","execution":{"iopub.status.busy":"2024-04-21T21:59:18.937483Z","iopub.execute_input":"2024-04-21T21:59:18.937805Z","iopub.status.idle":"2024-04-21T21:59:19.507056Z","shell.execute_reply.started":"2024-04-21T21:59:18.937776Z","shell.execute_reply":"2024-04-21T21:59:19.506193Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_images, val_images, train_labels, val_labels = train_test_split(\n    train_images, train_labels, test_size = 0.2, random_state = 42\n)","metadata":{"id":"evQtsDnugu3n","execution":{"iopub.status.busy":"2024-04-21T21:59:19.508207Z","iopub.execute_input":"2024-04-21T21:59:19.508507Z","iopub.status.idle":"2024-04-21T21:59:19.552536Z","shell.execute_reply.started":"2024-04-21T21:59:19.508481Z","shell.execute_reply":"2024-04-21T21:59:19.551638Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"'''\nclass BoundingBoxDatasetPaths(tf.keras.utils.Sequence):\n    def __init__(self, image_paths, annotation_paths, batch_size=4, shuffle=True):\n        self.image_paths = image_paths\n        self.annotation_paths = annotation_paths\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.indexes = np.arange(len(self.image_paths))\n\n    def __len__(self):\n        return len(self.image_paths) // self.batch_size // 16\n\n    def __getitem__(self, idx):\n        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        images = []\n        bboxes = []\n\n        for i in batch_indexes:\n            img = cv2.imread(self.image_paths[i])\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            images.append(img)\n\n            with open(self.annotation_paths[i], 'r') as f:\n                lines = f.readlines()\n                bbox = [float(x) for x in lines[0].split()]\n                bboxes.append(bbox)\n\n        images = np.array(images)\n        bboxes = np.array(bboxes)\n        \n       \n            \n        return images, bboxes\n        '''","metadata":{"execution":{"iopub.status.busy":"2024-04-21T21:59:19.553802Z","iopub.execute_input":"2024-04-21T21:59:19.554204Z","iopub.status.idle":"2024-04-21T21:59:19.562609Z","shell.execute_reply.started":"2024-04-21T21:59:19.554167Z","shell.execute_reply":"2024-04-21T21:59:19.561515Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"\"\\nclass BoundingBoxDatasetPaths(tf.keras.utils.Sequence):\\n    def __init__(self, image_paths, annotation_paths, batch_size=4, shuffle=True):\\n        self.image_paths = image_paths\\n        self.annotation_paths = annotation_paths\\n        self.batch_size = batch_size\\n        self.shuffle = shuffle\\n        self.indexes = np.arange(len(self.image_paths))\\n\\n    def __len__(self):\\n        return len(self.image_paths) // self.batch_size // 16\\n\\n    def __getitem__(self, idx):\\n        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\\n\\n        images = []\\n        bboxes = []\\n\\n        for i in batch_indexes:\\n            img = cv2.imread(self.image_paths[i])\\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\\n            images.append(img)\\n\\n            with open(self.annotation_paths[i], 'r') as f:\\n                lines = f.readlines()\\n                bbox = [float(x) for x in lines[0].split()]\\n                bboxes.append(bbox)\\n\\n        images = np.array(images)\\n        bboxes = np.array(bboxes)\\n        \\n       \\n            \\n        return images, bboxes\\n        \""},"metadata":{}}]},{"cell_type":"code","source":"!pip install -U torch torchvision\n!pip install yolov5 \nimport os\nimport cv2\nimport numpy as np\n\nclass BoundingBoxDataset(tf.keras.utils.Sequence):\n    def __init__(self, image_dir, annotation_dir, batch_size=4, shuffle=True):\n        #print(\"FEOFAF\")\n        self.image_paths = []\n        self.annotation_paths = []\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        #print(os.listdir(image_dir))\n        #print(os.listdir(annotation_dir))\n        # Iterate over the image directory\n        for img_path in os.listdir(image_dir):\n            image_path = os.path.join(image_dir, img_path)\n            annotation_path = os.path.join(annotation_dir, os.path.splitext(img_path)[0] + '.txt')\n\n            if os.path.isfile(image_path) and os.path.isfile(annotation_path):\n                self.image_paths.append(image_path)\n                self.annotation_paths.append(annotation_path)\n\n        self.indexes = np.arange(len(self.image_paths))\n        #print(len(self.image_paths),\"EEHE\")\n        #print(self.image_paths[5])\n\n    def __len__(self):\n       # print(\"ive been called\")\n        #print(len(self.image_paths),\"current image path length\")\n        #print(self.batch_size,\"current batch size\")\n        return len(self.image_paths) // self.batch_size\n\n    def __getitem__(self, idx):\n        #print(\"HELP\",idx)\n        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        images = []\n        bboxes = []\n\n        for i in batch_indexes:\n            img = cv2.imread(self.image_paths[i])\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            images.append(img)\n\n            with open(self.annotation_paths[i], 'r') as f:\n                lines = f.readlines()\n                if lines:\n                    bbox = [float(x) for x in lines[0].split()]\n                    bbox = bbox[1:]\n                    bboxes.append(bbox)\n                else:\n                    print(f\"Warning: Empty annotation file for image {self.image_paths[i]}\")\n                    bbox = [0,0,0,0]\n                    bboxes.append(bbox)\n                    continue\n        print(f\"Batch {idx}: Number of images = {len(images)}, Number of bboxes = {len(bboxes)}\")\n\n        if len(images) != self.batch_size or len(bboxes) != self.batch_size:\n            print(\"Warning: Inconsistent batch size\")\n        images = np.array(images)\n        bboxes = np.array(bboxes)\n\n        return images, bboxes\n\n \n \n\n\n#Defining the model architeture using basic convolution\n    #inputs\ninputs = tf.keras.Input(shape=(480,640,3))\n#tanh activation function\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Conv2D(32, (16, 16), padding='same', input_shape=(480, 640, 3))) #grab last layer\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Activation('relu'))\nmodel.add(tf.keras.layers.Conv2D(32, (16, 16), padding='same'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Activation('relu'))\nmodel.add(tf.keras.layers.MaxPooling2D(8,8))\nmodel.add(tf.keras.layers.Dropout(0.3))\nmodel.add(tf.keras.layers.Conv2D(64, (8, 8),padding='same'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Activation('relu'))\nmodel.add(tf.keras.layers.Conv2D(64, (8, 8), padding='same'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Activation('relu'))\nmodel.add(tf.keras.layers.MaxPooling2D(4,4))\nmodel.add(tf.keras.layers.Dropout(0.3))\nmodel.add(tf.keras.layers.Conv2D(128, (3, 3),  padding='same',))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Activation('relu'))\nmodel.add(tf.keras.layers.Conv2D(128, (3, 3), padding='same'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Activation('relu'))\nmodel.add(tf.keras.layers.MaxPooling2D(2,2))\nmodel.add(tf.keras.layers.Dropout(0.3))\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.Dense(units=2, activation='softmax'))\n#model = tf.keras.Model(inputs=inputs, outputs=outputs)\n#maybe modify dropout and amount of batch normalization we do\n#model summary\n#model.summary()\n\n#model = Model(inputs=inputs, outputs=outputs)\n\n#Freeze layers aka done training\n#for layer in backbone.layers:\n   # layer.trainable = False\n#!pip install tensorflow-addons\n#from tensorflow_addons.losses import huber_loss\nmodel.compile(optimizer='adam', loss=MeanSquaredError())\n\ndataset = BoundingBoxDataset(train_images_dir, train_labels_dir)\n\nprint(dataset)\nnum_epochs = 15\nfor epoch in range(num_epochs):\n    #images,bboxes = dataset.__getitem__()\n    #print(len(dataset),\"dsgajhgjsd\")\n    for images, bboxes in dataset:\n  \n        loss = model.train_on_batch(images, bboxes)\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}')\n\nmodel.save('new_triball_tracker.keras')\n# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"triball_tracker.json\", \"w\") as json_file:\n     json_file.write(model_json) \n \nmodel.save_weights(\"triball_tracker.weights.h5\")\nprint(\"Saved model to disk\") ","metadata":{"id":"tTsHRAM9Pgoh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"75a24475-d4c0-49fc-c60a-52bdc832280b","execution":{"iopub.status.busy":"2024-04-21T21:59:19.564376Z","iopub.execute_input":"2024-04-21T21:59:19.564792Z","iopub.status.idle":"2024-04-21T22:02:58.897299Z","shell.execute_reply.started":"2024-04-21T21:59:19.564756Z","shell.execute_reply":"2024-04-21T22:02:58.895417Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nCollecting torch\n  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\nCollecting torchvision\n  Downloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.2.0 (from torch)\n  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nDownloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2\n    Uninstalling torch-2.1.2:\n      Successfully uninstalled torch-2.1.2\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.16.2\n    Uninstalling torchvision-0.16.2:\n      Successfully uninstalled torchvision-0.16.2\nSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 torchvision-0.17.2 triton-2.2.0\nCollecting yolov5\n  Downloading yolov5-7.0.13-py37.py38.py39.py310-none-any.whl.metadata (10 kB)\nRequirement already satisfied: gitpython>=3.1.30 in /opt/conda/lib/python3.10/site-packages (from yolov5) (3.1.41)\nRequirement already satisfied: matplotlib>=3.3 in /opt/conda/lib/python3.10/site-packages (from yolov5) (3.7.5)\nRequirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from yolov5) (1.26.4)\nRequirement already satisfied: opencv-python>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from yolov5) (4.9.0.80)\nRequirement already satisfied: Pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from yolov5) (9.5.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from yolov5) (5.9.3)\nRequirement already satisfied: PyYAML>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from yolov5) (6.0.1)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from yolov5) (2.31.0)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from yolov5) (1.11.4)\nCollecting thop>=0.1.1 (from yolov5)\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: torch>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from yolov5) (2.2.2)\nRequirement already satisfied: torchvision>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from yolov5) (0.17.2)\nRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from yolov5) (4.66.1)\nCollecting ultralytics>=8.0.100 (from yolov5)\n  Downloading ultralytics-8.2.2-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m517.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorboard>=2.4.1 in /opt/conda/lib/python3.10/site-packages (from yolov5) (2.15.1)\nRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from yolov5) (2.1.4)\nRequirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from yolov5) (0.12.2)\nRequirement already satisfied: setuptools>=65.5.1 in /opt/conda/lib/python3.10/site-packages (from yolov5) (69.0.3)\nCollecting fire (from yolov5)\n  Downloading fire-0.6.0.tar.gz (88 kB)\n\u001b[2K     \u001b[90m\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: boto3>=1.19.1 in /opt/conda/lib/python3.10/site-packages (from yolov5) (1.26.100)\nCollecting sahi>=0.11.10 (from yolov5)\n  Downloading sahi-0.11.15-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: huggingface-hub>=0.12.0 in /opt/conda/lib/python3.10/site-packages (from yolov5) (0.22.2)\nCollecting roboflow>=0.2.29 (from yolov5)\n  Downloading roboflow-1.1.27-py3-none-any.whl.metadata (9.3 kB)\nCollecting botocore<1.30.0,>=1.29.100 (from boto3>=1.19.1->yolov5)\n  Downloading botocore-1.29.165-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.19.1->yolov5) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.19.1->yolov5) (0.6.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython>=3.1.30->yolov5) (4.0.11)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.12.0->yolov5) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.12.0->yolov5) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.12.0->yolov5) (21.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.12.0->yolov5) (4.9.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->yolov5) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->yolov5) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->yolov5) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->yolov5) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->yolov5) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->yolov5) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->yolov5) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->yolov5) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->yolov5) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->yolov5) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->yolov5) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->yolov5) (2024.2.2)\nCollecting certifi>=2017.4.17 (from requests>=2.23.0->yolov5)\n  Downloading certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\nCollecting chardet==4.0.0 (from roboflow>=0.2.29->yolov5)\n  Downloading chardet-4.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\nCollecting cycler>=0.10 (from matplotlib>=3.3->yolov5)\n  Downloading cycler-0.10.0-py2.py3-none-any.whl.metadata (722 bytes)\nCollecting idna<4,>=2.5 (from requests>=2.23.0->yolov5)\n  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\nCollecting opencv-python-headless==4.8.0.74 (from roboflow>=0.2.29->yolov5)\n  Downloading opencv_python_headless-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\nRequirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (from roboflow>=0.2.29->yolov5) (1.0.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from roboflow>=0.2.29->yolov5) (1.16.0)\nRequirement already satisfied: requests-toolbelt in /opt/conda/lib/python3.10/site-packages (from roboflow>=0.2.29->yolov5) (0.10.1)\nCollecting python-magic (from roboflow>=0.2.29->yolov5)\n  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\nCollecting opencv-python>=4.1.1 (from yolov5)\n  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: shapely>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from sahi>=0.11.10->yolov5) (1.8.5.post1)\nCollecting pybboxes==0.1.6 (from sahi>=0.11.10->yolov5)\n  Downloading pybboxes-0.1.6-py3-none-any.whl.metadata (9.9 kB)\nCollecting terminaltables (from sahi>=0.11.10->yolov5)\n  Downloading terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from sahi>=0.11.10->yolov5) (8.1.7)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (3.5.2)\nRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (3.20.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (3.0.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->yolov5) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->yolov5) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->yolov5) (3.1.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->yolov5) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->yolov5) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->yolov5) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->yolov5) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->yolov5) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->yolov5) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->yolov5) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->yolov5) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->yolov5) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->yolov5) (2.19.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->yolov5) (12.1.105)\nRequirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->yolov5) (2.2.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.0->yolov5) (12.4.127)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics>=8.0.100->yolov5) (9.0.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire->yolov5) (2.4.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->yolov5) (5.0.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.4.1->yolov5) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->yolov5) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7.0->yolov5) (1.3.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.4.1->yolov5) (3.2.2)\nDownloading yolov5-7.0.13-py37.py38.py39.py310-none-any.whl (953 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m953.4/953.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading roboflow-1.1.27-py3-none-any.whl (74 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m74.1/74.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\nDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opencv_python_headless-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.1 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n\u001b[?25hDownloading sahi-0.11.15-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pybboxes-0.1.6-py3-none-any.whl (24 kB)\nDownloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\nDownloading ultralytics-8.2.2-py3-none-any.whl (750 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m750.8/750.8 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading botocore-1.29.165-py3-none-any.whl (11.0 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\nDownloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\nBuilding wheels for collected packages: fire\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117031 sha256=0cebe76f9aa4c0ff109337ecd2f8b47a6d9c86d549f33bc1b8f05967336dbbb8\n  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\nSuccessfully built fire\nInstalling collected packages: terminaltables, python-magic, pybboxes, opencv-python-headless, opencv-python, idna, fire, cycler, chardet, certifi, botocore, sahi, thop, roboflow, ultralytics, yolov5\n  Attempting uninstall: opencv-python-headless\n    Found existing installation: opencv-python-headless 4.9.0.80\n    Uninstalling opencv-python-headless-4.9.0.80:\n      Successfully uninstalled opencv-python-headless-4.9.0.80\n  Attempting uninstall: opencv-python\n    Found existing installation: opencv-python 4.9.0.80\n    Uninstalling opencv-python-4.9.0.80:\n      Successfully uninstalled opencv-python-4.9.0.80\n  Attempting uninstall: idna\n    Found existing installation: idna 3.6\n    Uninstalling idna-3.6:\n      Successfully uninstalled idna-3.6\n  Attempting uninstall: cycler\n    Found existing installation: cycler 0.12.1\n    Uninstalling cycler-0.12.1:\n      Successfully uninstalled cycler-0.12.1\n  Attempting uninstall: certifi\n    Found existing installation: certifi 2024.2.2\n    Uninstalling certifi-2024.2.2:\n      Successfully uninstalled certifi-2024.2.2\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.34.51\n    Uninstalling botocore-1.34.51:\n      Successfully uninstalled botocore-1.34.51\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\naiobotocore 2.12.2 requires botocore<1.34.52,>=1.34.41, but you have botocore 1.29.165 which is incompatible.\nalbumentations 1.4.0 requires opencv-python>=4.9.0, but you have opencv-python 4.7.0.72 which is incompatible.\njupyterlab 4.1.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.1.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed botocore-1.29.165 certifi-2023.7.22 chardet-4.0.0 cycler-0.10.0 fire-0.6.0 idna-2.10 opencv-python-4.7.0.72 opencv-python-headless-4.8.0.74 pybboxes-0.1.6 python-magic-0.4.27 roboflow-1.1.27 sahi-0.11.15 terminaltables-3.1.10 thop-0.1.1.post2209072238 ultralytics-8.2.2 yolov5-7.0.13\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(\n","output_type":"stream"},{"name":"stdout","text":"<__main__.BoundingBoxDataset object at 0x7c32d44d13f0>\nBatch 0: Number of images = 4, Number of bboxes = 4\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 126\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m#images,bboxes = dataset.__getitem__()\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m#print(len(dataset),\"dsgajhgjsd\")\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, bboxes \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m--> 126\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    129\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew_triball_tracker.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:551\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata\u001b[39m():\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (x, y, sample_weight)\n\u001b[0;32m--> 551\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m logs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x), logs)\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:118\u001b[0m, in \u001b[0;36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step given a Dataset iterator.\"\"\"\u001b[39;00m\n\u001b[1;32m    117\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[0;32m--> 118\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mone_step_on_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[1;32m    122\u001b[0m     outputs,\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[1;32m    124\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_reduction_method,\n\u001b[1;32m    125\u001b[0m )\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:106\u001b[0m, in \u001b[0;36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step_on_data\u001b[39m(data):\n\u001b[1;32m    105\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:60\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x)\n\u001b[0;32m---> 60\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_tracker\u001b[38;5;241m.\u001b[39mupdate_state(loss)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/trainers/trainer.py:322\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    320\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compile_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 322\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m         losses\u001b[38;5;241m.\u001b[39mappend(loss)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/trainers/compile_utils.py:605\u001b[0m, in \u001b[0;36mCompileLoss.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname):\n\u001b[0;32m--> 605\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/trainers/compile_utils.py:641\u001b[0m, in \u001b[0;36mCompileLoss.call\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loss, y_t, y_p, loss_weight, sample_weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflat_losses,\n\u001b[1;32m    634\u001b[0m     y_true,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    637\u001b[0m     sample_weight,\n\u001b[1;32m    638\u001b[0m ):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss:\n\u001b[1;32m    640\u001b[0m         value \u001b[38;5;241m=\u001b[39m loss_weight \u001b[38;5;241m*\u001b[39m ops\u001b[38;5;241m.\u001b[39mcast(\n\u001b[0;32m--> 641\u001b[0m             \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mfloatx()\n\u001b[1;32m    642\u001b[0m         )\n\u001b[1;32m    643\u001b[0m         loss_values\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_values:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/losses/loss.py:42\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m     35\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype), y_pred\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m y_true \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype), y_true\n\u001b[1;32m     40\u001b[0m )\n\u001b[0;32m---> 42\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m out_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(losses, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/losses/losses.py:22\u001b[0m, in \u001b[0;36mLossFunctionWrapper.call\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_true, y_pred):\n\u001b[1;32m     21\u001b[0m     y_true, y_pred \u001b[38;5;241m=\u001b[39m squeeze_or_expand_to_same_rank(y_true, y_pred)\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/losses/losses.py:1154\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m   1152\u001b[0m y_true \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(y_true, dtype\u001b[38;5;241m=\u001b[39my_pred\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   1153\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m squeeze_or_expand_to_same_rank(y_true, y_pred)\n\u001b[0;32m-> 1154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mmean(ops\u001b[38;5;241m.\u001b[39msquare(\u001b[43my_true\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 4 and 2 for '{{node compile_loss/mean_squared_error/sub}} = Sub[T=DT_FLOAT](compile_loss/mean_squared_error/Cast, sequential_1/dense_1_2/Softmax)' with input shapes: [4,4], [4,2]."],"ename":"ValueError","evalue":"Dimensions must be equal, but are 4 and 2 for '{{node compile_loss/mean_squared_error/sub}} = Sub[T=DT_FLOAT](compile_loss/mean_squared_error/Cast, sequential_1/dense_1_2/Softmax)' with input shapes: [4,4], [4,2].","output_type":"error"}]},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # dimesion reduction\n        rotation_range=10,  # randomly rotate images in the range 5 degrees\n        zoom_range = 0.1, # Randomly zoom image 10%\n        width_shift_range=0.1,  # randomly shift images horizontally 10%\n        height_shift_range=0.1,  # randomly shift images vertically 10%\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(train_images)\n\nfor img_path in os.listdir(train_images_dir):  # Assume you have a list of image paths\n    img = load_img(img_path)\n    x = img_to_array(img)\n    x = x.reshape((1,) + x.shape)\n\n    i = 0\n    for batch in datagen.flow(x, batch_size=1, save_to_dir='path_to_save', save_prefix='aug', save_format='jpeg'):\n        i += 1\n        if i > 52:\n            break","metadata":{"execution":{"iopub.status.busy":"2024-04-21T22:02:58.898720Z","iopub.status.idle":"2024-04-21T22:02:58.899157Z","shell.execute_reply.started":"2024-04-21T22:02:58.898971Z","shell.execute_reply":"2024-04-21T22:02:58.898988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimage_dir = '/kaggle/input/unlabeledtriballs/data_old'\n\nfor filename in os.listdir(image_dir):\n    #Sanity check\n     if filename.endswith('.jpg') or filename.endswith('.png'):\n        image_path = os.path.join(image_dir, filename)\n        \n        # Generic preprocessing\n        img = cv2.imread(image_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        input_img = np.expand_dims(img, axis=0)\n        \n        predictions = model.predict(input_img)\n\n\n        for bbox in predictions[0]:\n            if type(bbox.item()) == type(0.0):\n                print(\"skip \",bbox)\n                continue\n            \n            x1, y1, x2, y2, _ = bbox\n            print(x1)\n            print(y1)\n            print(x2)\n            print(y2)\n            print(bbox)\n            #x1 = int(x1 * img.shape[1])\n            #y1 = int(y1 * img.shape[0])\n            #x2 = int(x2 * img.shape[1])\n           # y2 = int(y2 * img.shape[0])\n            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n        #testing code frankly\n       # cv2.imshow('Image with Bounding Boxes', img)\n       # cv2.waitKey(0)\n       # cv2.destroyAllWindows()","metadata":{"id":"RWAvrJHEhSq4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"229a7d26-1971-4416-e14a-8fd008abff01","execution":{"iopub.status.busy":"2024-04-21T22:02:58.900728Z","iopub.status.idle":"2024-04-21T22:02:58.901138Z","shell.execute_reply.started":"2024-04-21T22:02:58.900958Z","shell.execute_reply":"2024-04-21T22:02:58.900975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#model.compile(optimizer='adam',\n              #loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n          #    metrics=['accuracy'])\n#3\n#history = model.fit(train_images, train_labels, epochs=10,\n               #     validation_data=(test_images, test_labels))\n","metadata":{"id":"UZcP0R-0Pgoh","execution":{"iopub.status.busy":"2024-04-21T22:02:58.902881Z","iopub.status.idle":"2024-04-21T22:02:58.903289Z","shell.execute_reply.started":"2024-04-21T22:02:58.903100Z","shell.execute_reply":"2024-04-21T22:02:58.903117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_datetime = datetime.datetime.now().strftime('%Y_%m_%d_%H_%S')\nfilepath = f'./model_{file_datetime}_{device_name_for_file}.keras'\nprint(f'Best model will be saved to: {Path.cwd() / filepath}')","metadata":{"id":"4OcouJIYPgoh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"41605bfb-c24b-4201-fba1-bb07321257a0","execution":{"iopub.status.busy":"2024-04-21T22:02:58.905068Z","iopub.status.idle":"2024-04-21T22:02:58.905597Z","shell.execute_reply.started":"2024-04-21T22:02:58.905324Z","shell.execute_reply":"2024-04-21T22:02:58.905347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model, using datagen data\n\n'''\nBATCH_SIZE = 4\nNUM_EPOCHS = 30\n#opt_rms = tf.keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\nMIN_VAL_LOSS = 0#np.inf#bad name, trying now to optimize accuracy\nloss_history = [[],[]]\nacc_history = [[],[]]\ntotal_train_time = 0.0\nmodel.compile(loss='categorical_crossentropy',\n        optimizer='adam', #maybe change optimizer?\n        metrics=['accuracy'])\nkf = KFold(n_splits=10, shuffle=True, random_state=42)#maybe use? maybe change to 5?\n# Training loop\n# NOTE: This works better than callbacks\nfor i in range(0, NUM_EPOCHS):\n    print(f'EPOCH: {i+1}/{NUM_EPOCHS}')\n    start_epoch = time.time()\n\n    history = model.fit(\n        train_images,train_labels,\n        #datagen.flow(train_images,train_labels_cat, batch_size=BATCH_SIZE),\n        validation_data=(val_images, val_labels),\n        epochs=1,\n        verbose = 1\n    )\n    end_epoch = time.time()#https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png\n    delta_epoch = end_epoch - start_epoch\n    total_train_time += delta_epoch\n    print(f'LOG --> This epoch took {delta_epoch}...')\n    if history.history['val_accuracy'][0] > MIN_VAL_LOSS: #val_accuracy remember to switch the comparison when switching\n        print(f'LOG --> val_accuracy improved from {MIN_VAL_LOSS} to {history.history[\"val_accuracy\"][0]}...')\n        print(f'LOG --> saving model as {filepath}')\n        MIN_VAL_LOSS = history.history['val_accuracy'][0]\n        model.save(filepath, overwrite=True)\n    else:\n        print(f'LOG --> val_accuracy did not improve...')\n    # Keep track of the training history\n    loss_history[0].append(history.history['loss'][0])\n    loss_history[1].append(history.history['val_loss'][0])\n    acc_history[0].append(history.history['accuracy'][0])\n    acc_history[1].append(history.history['val_accuracy'][0])\nprint(f'LOG--> Total training time: {total_train_time}')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":108},"id":"Fwm8UlWjPgoh","outputId":"83db78e7-d078-439d-e39d-ea996b1e01bb","execution":{"iopub.status.busy":"2024-04-21T22:02:58.907799Z","iopub.status.idle":"2024-04-21T22:02:58.908179Z","shell.execute_reply.started":"2024-04-21T22:02:58.908000Z","shell.execute_reply":"2024-04-21T22:02:58.908016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''plt.plot(acc_history[0], color='black',)\nplt.plot(acc_history[1], color='red',)\nplt.title('Training Accuracy vs Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='lower right')\nplt.show()\n\nplt.plot(loss_history[0])\nplt.plot(loss_history[1])\nplt.title('Training Loss vs Validation Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\nplt.show()","metadata":{"id":"DOa1a6gyPgoi","execution":{"iopub.status.busy":"2024-04-21T22:02:58.909503Z","iopub.status.idle":"2024-04-21T22:02:58.909987Z","shell.execute_reply.started":"2024-04-21T22:02:58.909736Z","shell.execute_reply":"2024-04-21T22:02:58.909756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import csv\n\n# Load the best model\nprint(f'Loading saved model from {filepath}...')\nbest_model = tf.keras.models.load_model(filepath)\n\nclasses = {\n    0: 'airplane',\n    1: 'car',\n    2: 'bird',\n    3: 'cat',\n    4: 'deer',\n    5: 'dog',\n    6: 'frog',\n    7: 'horse',\n    8: 'ship',\n    9: 'truck'\n}\n\n# Warm up device\nprint('Starting Warmup...')\nwith open('submission.csv', 'w', newline='\\n') as csvfile:\n    writer = csv.writer(csvfile)\n    count = 1\n    writer.writerow(['id','label'])\n    for prediction in best_model.predict(test_images, batch_size=BATCH_SIZE, verbose=0):\n        value = np.argmax(prediction)\n        writer.writerow([count,classes[value]])\n        count+=1\n\n","metadata":{"id":"wmRiKiq_Pgoi","execution":{"iopub.status.busy":"2024-04-21T22:02:58.911633Z","iopub.status.idle":"2024-04-21T22:02:58.912114Z","shell.execute_reply.started":"2024-04-21T22:02:58.911875Z","shell.execute_reply":"2024-04-21T22:02:58.911896Z"},"trusted":true},"execution_count":null,"outputs":[]}]}